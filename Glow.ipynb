{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_bins = 10\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.Compose([transforms.ToTensor(), lambda x: x + torch.zeros_like(x).uniform_(0., 1./n_bins)]),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.initialized = False\n",
    "        self.weights = nn.Parameter(torch.Tensor(c))\n",
    "        self.bias = nn.Parameter(torch.Tensor(c))\n",
    "#         self.weights.data.uniform_(-0.1, 0.1)\n",
    "#         self.bias.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, inp, logdet):\n",
    "        if not self.initialized:\n",
    "            c = self.shape[-1]\n",
    "            self.weights.data = 1/inp.transpose(0, 1).contiguous().view(c, -1).std(1)\n",
    "            self.bias.data = -(inp * self.weights[..., None, None]).transpose(0, 1).contiguous().view(c, -1).mean(1)\n",
    "            self.initialized = True\n",
    "        \n",
    "        return inp * self.weights[..., None, None] + self.bias[..., None, None], logdet + self.shape[0] * self.shape[1] * torch.sum(torch.abs(self.weights))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return (out - self.bias) / self.weights\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(np.linalg.qr(np.random.randn(c, c))[0]).float())\n",
    "                                    \n",
    "    def forward(self, inp, logdet):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, self.weight)), logdet + self.shape[0] * self.shape[1] * torch.log(torch.abs(torch.det(self.weight)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, torch.inverse(self.weight)))\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(c//2, c//2, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c//2, c, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp, logdet):        \n",
    "        x_a, x_b = torch.split(inp, inp.shape[1] // 2, dim=1)\n",
    "        log_s, t = torch.split(self.NN(x_b), inp.shape[1]//2, 1)\n",
    "        s = torch.exp(log_s)\n",
    "        y_a = s * x_a + t\n",
    "        return torch.cat([y_a, x_b], dim = 1), logdet + torch.sum(torch.log(torch.abs(s)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        y_a, y_b = torch.split(out, out.shape[1] // 2, dim = 1)\n",
    "        log_s, t = torch.split(self.NN(y_b), inp.shape[1]//2, 1)\n",
    "        s = torch.exp(log_s)\n",
    "        x_a = (y_a - t) / s\n",
    "        return torch.cat([x_a, y_b], dim = 1)\n",
    "    \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, K, h, w, c):\n",
    "        super().__init__()\n",
    "        self.k = K\n",
    "        self.actnorms = nn.ModuleList(ActNorm(h, w, c) for i in range(K))\n",
    "        self.invconvs = nn.ModuleList(InvertibleConv(h, w, c) for i in range(K))\n",
    "        self.couplings = nn.ModuleList(AffineCoupling(h, w, c) for i in range(K))\n",
    "        \n",
    "    def forward(self, inp, logdet, z):        \n",
    "        for i in range(self.k):\n",
    "            out, logdet = self.actnorms[i](inp, logdet)\n",
    "            out, logdet = self.invconvs[i](out, logdet)\n",
    "            out, logdet = self.couplings[i](out, logdet)\n",
    "        \n",
    "        return out, logdet, z\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        out = self.actnorm.reverse(out)\n",
    "        out = self.invconv.reverse(out)\n",
    "        out = self.coupling.reverse(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle(input_, block_size=2):\n",
    "    b, c, h, w = input_.shape\n",
    "\n",
    "    assert h % block_size == 0 and w % block_size == 0,\\\n",
    "        f\"Shape must be divisible by block_size, got {input_.shape}\"\n",
    "\n",
    "    oc = c * block_size * block_size;\n",
    "    oh = h // block_size;\n",
    "    ow = w // block_size;\n",
    "\n",
    "    input_reshaped = input_.view(b, c, oh, block_size, ow, block_size)\n",
    "    return input_reshaped.permute(0, 1, 3, 5, 2, 4).reshape(b, oc, oh, ow)\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, block_size=2):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size \n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        return pixel_unshuffle(input_, self.block_size), logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return F.pixel_shuffle(input_, self.block_size), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(nn.Module):\n",
    "    def forward(self, input_, logdet, z):\n",
    "        out, zi = torch.split(input_, input_.shape[1]//2, 1)\n",
    "\n",
    "        return out, logdet, z + [zi]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        out = torch.cat([input_, z[-1]], 1)\n",
    "        \n",
    "        return out, z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikelihood(nn.Module): \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.norm = torch.distributions.MultivariateNormal(torch.zeros(size).to(device), torch.eye(size).to(device))\n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        batch = input_.shape[0]\n",
    "        comb_z = torch.cat([z_.contiguous().view(batch, -1) for z_ in z + [input_]], 1)\n",
    "\n",
    "        return None, logdet + torch.sum(self.norm.log_prob(comb_z)), comb_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowModel(nn.Module):\n",
    "    def __init__(self, h, w, c, K, L):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            self.layers.append(Squeeze())\n",
    "            self.layers.append(Flow(K, h//(2**i), w//(2**i), c*2**(i+1)))\n",
    "            self.layers.append(Split())\n",
    "\n",
    "        i += 1\n",
    "        self.layers.append(Squeeze())\n",
    "        self.layers.append(Flow(K, h//(2**i), w//(2**i), c*2**(i+1)))\n",
    "        self.layers.append(LogLikelihood(h*w*c))\n",
    "        self.k = K\n",
    "        self.l = L\n",
    "        \n",
    "    def forward(self, input_, logdet, z):\n",
    "        for layer in self.layers:\n",
    "            input_, logdet, z = layer(input_, logdet, z)\n",
    "            \n",
    "        return input_, logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        for layer in self.layers[::-1]:\n",
    "            input_, z = layer.reverse(input_, z)\n",
    "\n",
    "        return input_, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlowModel(28, 28, 1, 8, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, logdet, z = model.forward(next(iter(train_dataset))[0][None, ...].to(device), 0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.train()\n",
    "    batch_iter =tqdm(train_loader)\n",
    "    for x_batch, _ in batch_iter:\n",
    "        data = x_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, logdet, z = model(data, 0, [])\n",
    "        \n",
    "        loss = -logdet + np.log(n_bins) * np.prod(x_batch.shape)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        batch_iter.set_postfix(loss=loss)\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, batchsize=batch_size)\n",
    "        train_log.extend(train_loss)\n",
    "\n",
    "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
    "\n",
    "        clear_output()\n",
    "        plot_history(train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 17/235 [00:03<00:40,  5.36it/s, loss=-inf]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0b8cd87ff629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-2bd6db670c56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, opt, n_epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2bd6db670c56>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, batchsize)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlogdet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optim.Adam(model.parameters()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
