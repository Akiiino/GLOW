{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.initialized = False\n",
    "        self.weights = nn.Parameter(torch.Tensor(c))\n",
    "        self.bias = nn.Parameter(torch.Tensor(c))\n",
    "#         self.weights.data.uniform_(-0.1, 0.1)\n",
    "#         self.bias.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, inp, logdet):\n",
    "        if not self.initialized:\n",
    "            c = self.shape[-1]\n",
    "            self.weights.data = 1/inp.transpose(0, 1).contiguous().view(c, -1).std(1)\n",
    "            self.bias.data = -(inp * self.weights[..., None, None]).transpose(0, 1).contiguous().view(c, -1).mean(1)\n",
    "            self.initialized = True\n",
    "        \n",
    "        return inp * self.weights[..., None, None] + self.bias[..., None, None], logdet + self.shape[0] * self.shape[1] * torch.sum(torch.abs(self.weights))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return (out - self.bias) / self.weights\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(np.linalg.qr(np.random.randn(c, c))[0]).float())\n",
    "                                    \n",
    "    def forward(self, inp, logdet):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, self.weight)), logdet + self.shape[0] * self.shape[1] * torch.log(torch.abs(torch.det(self.weight)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, torch.inverse(self.weight)))\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(c//2, c//2, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c//2, c, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp, logdet):        \n",
    "        x_a, x_b = torch.split(inp, inp.shape[1] // 2, dim=1)\n",
    "        log_s, t = torch.split(self.NN(x_b), inp.shape[1]//2, 1)\n",
    "        s = torch.exp(log_s)\n",
    "        y_a = s * x_a + t\n",
    "        return torch.cat([y_a, x_b], dim = 1), logdet + torch.sum(torch.log(torch.abs(s)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        y_a, y_b = torch.split(out, out.shape[1] // 2, dim = 1)\n",
    "        log_s, t = torch.split(self.NN(y_b), inp.shape[1]//2, 1)\n",
    "        s = torch.exp(log_s)\n",
    "        x_a = (y_a - t) / s\n",
    "        return torch.cat([x_a, y_b], dim = 1)\n",
    "    \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, K, h, w, c):\n",
    "        super().__init__()\n",
    "        self.k = K\n",
    "        self.actnorms = nn.ModuleList(ActNorm(h, w, c) for i in range(K))\n",
    "        self.invconvs = nn.ModuleList(InvertibleConv(h, w, c) for i in range(K))\n",
    "        self.couplings = nn.ModuleList(AffineCoupling(h, w, c) for i in range(K))\n",
    "        \n",
    "    def forward(self, inp, logdet, z):        \n",
    "        for i in range(self.k):\n",
    "            out, logdet = self.actnorms[i](inp, logdet)\n",
    "            out, logdet = self.invconvs[i](out, logdet)\n",
    "            out, logdet = self.couplings[i](out, logdet)\n",
    "        \n",
    "        return out, logdet, z\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        out = self.actnorm.reverse(out)\n",
    "        out = self.invconv.reverse(out)\n",
    "        out = self.coupling.reverse(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle(input_, block_size=2):\n",
    "    b, c, h, w = input_.shape\n",
    "\n",
    "    assert h % block_size == 0 and w % block_size == 0,\\\n",
    "        f\"Shape must be divisible by block_size, got {input_.shape}\"\n",
    "\n",
    "    oc = c * block_size * block_size;\n",
    "    oh = h // block_size;\n",
    "    ow = w // block_size;\n",
    "\n",
    "    input_reshaped = input_.view(b, c, oh, block_size, ow, block_size)\n",
    "    return input_reshaped.permute(0, 1, 3, 5, 2, 4).reshape(b, oc, oh, ow)\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, block_size=2):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size \n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        return pixel_unshuffle(input_, self.block_size), logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return F.pixel_shuffle(input_, self.block_size), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(nn.Module):\n",
    "    def forward(self, input_, logdet, z):\n",
    "        out, zi = torch.split(input_, input_.shape[1]//2, 1)\n",
    "\n",
    "        return out, logdet, z + [zi]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        out = torch.cat([input_, z[-1]], 1)\n",
    "        \n",
    "        return out, z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikelihood(nn.Module): \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.norm = torch.distributions.MultivariateNormal(torch.zeros(size).to(device), torch.eye(size).to(device))\n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        batch = input_.shape[0]\n",
    "        comb_z = torch.cat([z_.contiguous().view(batch, -1) for z_ in z + [input_]], 1)\n",
    "\n",
    "        return None, logdet + torch.sum(self.norm.log_prob(comb_z)), comb_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowModel(nn.Module):\n",
    "    def __init__(self, h, w, c, K, L):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            self.layers.append(Squeeze())\n",
    "            self.layers.append(Flow(K, h//(2**i), w//(2**i), c*2**(i+1)))\n",
    "            self.layers.append(Split())\n",
    "\n",
    "        i += 1\n",
    "        self.layers.append(Squeeze())\n",
    "        self.layers.append(Flow(K, h//(2**i), w//(2**i), c*2**(i+1)))\n",
    "        self.layers.append(LogLikelihood(h*w*c))\n",
    "        self.k = K\n",
    "        self.l = L\n",
    "        \n",
    "    def forward(self, input_, logdet, z):\n",
    "        for layer in self.layers:\n",
    "            input_, logdet, z = layer(input_, logdet, z)\n",
    "            \n",
    "        return input_, logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        for layer in self.layers[::-1]:\n",
    "            input_, z = layer.reverse(input_, z)\n",
    "\n",
    "        return input_, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlowModel(28, 28, 1, 8, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, logdet, z = model.forward(next(iter(train_dataset))[0][None, ...].to(device), 0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.train()\n",
    "    batch_iter =tqdm(train_loader)\n",
    "    for x_batch, _ in batch_iter:\n",
    "        data = x_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, logdet, z = model(data, 0, [])\n",
    "\n",
    "        loss = -logdet\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        batch_iter.set_postfix(loss=loss)\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    batchsize = 32\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, batchsize=batchsize)\n",
    "        train_log.extend(train_loss)\n",
    "\n",
    "        steps = train_dataset.train_labels.shape[0] / batch_size\n",
    "\n",
    "        clear_output()\n",
    "        plot_history(train_log)    \n",
    "            \n",
    "    print(\"Final error: {:.2%}\".format(1 - val_acc_log[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd0f10a95d449f499b658db2630052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=469), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, optim.Adam(model.parameters()), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
