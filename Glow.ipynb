{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNIST/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.initialized = False\n",
    "        self.weights = nn.Parameter(torch.Tensor(c))\n",
    "        self.bias = nn.Parameter(torch.Tensor(c))\n",
    "#         self.weights.data.uniform_(-0.1, 0.1)\n",
    "#         self.bias.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, inp, logdet):\n",
    "        if not self.initialized:\n",
    "            c = self.shape[-1]\n",
    "            self.weights.data = 1/inp.view(-1, c).std(0)\n",
    "            self.bias.data = -(inp * self.weights).view(-1, c).mean(0)\n",
    "            self.initialized = True\n",
    "        \n",
    "        return inp * self.weights + self.bias, logdet + self.shape[0] * self.shape[1] * torch.log(torch.abs(self.weights))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return (out - self.bias) / self.weights\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(np.linalg.qr(np.random.randn(c, c))[0]))\n",
    "                                    \n",
    "    def forward(self, inp, logdet):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, self.weight)), logdet + self.shape[0] * self.shape[1] * torch.log(torch.abs(torch.det(self.weight)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, torch.inverse(self.weight)))\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Conv2d(c, c, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c, c, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp, logdet):\n",
    "        x_a, x_b = torch.split(inp, inp.shape[1] // 2, dim = 1)\n",
    "        log_s, t = self.NN(x_b)\n",
    "        s = torch.exp(log_s)\n",
    "        y_a = s * x_a + t\n",
    "        return torch.cat([y_a, x_b], dim = 1), logdet + torch.sum(torch.log(torch.abs(s)))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        y_a, y_b = torch.split(out, out.shape[1] // 2, dim = 1)\n",
    "        log_s, t = self.NN(y_b)\n",
    "        s = torch.exp(log_s)\n",
    "        x_a = (y_a - t) / s\n",
    "        return torch.cat([x_a, y_b], dim = 1)\n",
    "    \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, K, h, w, c):\n",
    "        super().__init__()\n",
    "        self.k = K\n",
    "        self.actnorms = [ActNorm(h, w, c) for i in range(K)]\n",
    "        self.invconvs = [InvertibleConv(h, w, c) for i in range(K)]\n",
    "        self.couplings = [AffineCoupling(h, w, c) for i in range(K)]\n",
    "        \n",
    "    def forward(self, inp, logdet, z):        \n",
    "        for i in range(self.k):\n",
    "            out, logdet = self.actnorm[i](inp, logdet)\n",
    "            out, logdet = self.invconv[i](out, logdet)\n",
    "            out, logdet = self.coupling[i](out, logdet)\n",
    "        \n",
    "        return out, logdet, z\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        out = self.actnorm.reverse(out)\n",
    "        out = self.invconv.reverse(out)\n",
    "        out = self.coupling.reverse(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle(input_, block_size=2):\n",
    "    b, c, h, w = input_.shape\n",
    "\n",
    "    assert h % block_size == 0 and w % block_size == 0,\\\n",
    "        f\"Shape must be divisible by block_size, got {input_.shape}\"\n",
    "\n",
    "    oc = c * block_size * block_size;\n",
    "    oh = h // block_size;\n",
    "    ow = w // block_size;\n",
    "\n",
    "    input_reshaped = input_.view(b, c, oh, block_size, ow, block_size)\n",
    "    return input_reshaped.permute(0, 1, 3, 5, 2, 4).reshape(b, oc, oh, ow)\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, block_size=2):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size \n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        return pixel_unshuffle(input_, self.block_size), logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return F.pixel_shuffle(input_, self.block_size), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(nn.Module):\n",
    "    def forward(self, input_, logdet, z):\n",
    "        out, zi = torch.split(input_, input_.shape[1]//2, 1)\n",
    "\n",
    "        return out, logdet, z + [zi]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        out = torch.cat([input_, z[-1]], 1)\n",
    "        \n",
    "        return out, z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowModel(nn.Module):\n",
    "    def __init__(self, NN, h, w, c, K, L):\n",
    "        super().__init__()\n",
    "#         self.flows = [[Flow(NN, h, w, int(c / 2 ** j)) for i in range(K)] for j in range(L)]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(L-1):\n",
    "            self.layers.append(Squeeze())\n",
    "            self.layers.append(Flow(h//(2**i), w//(2**i), c*4**i))\n",
    "            self.layers.append(Split())\n",
    "\n",
    "        self.layers.append(Squeeze())\n",
    "        self.layers.append(Flow(h//(2**i), w//(2**i), c*4**i))\n",
    "        \n",
    "        self.k = K\n",
    "        self.l = L\n",
    "        \n",
    "    def forward(self, input_, logdet, z):\n",
    "        for layer in self.layers:\n",
    "            input_, logdet, z = layer(input_, logdet, z)\n",
    "            \n",
    "        return input_, logdet, z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
