{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_bins = 2 ** 8\n",
    "\n",
    "train_dataset = dsets.MNIST(\n",
    "    root='./MNIST/', \n",
    "    train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x.to(device)# / n_bins - 0.5 + torch.zeros_like(x).uniform_(0, 1. / n_bins)\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "#     num_workers=8,\n",
    "#     pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeirdConv(nn.Module): \n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_, out, 3, padding = 1)\n",
    "        self.logfactor = nn.Parameter(torch.zeros((1, out, 1, 1)))\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        out = self.conv(input_)\n",
    "        out *= torch.exp(self.logfactor)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroBiasInitConv(nn.Module): \n",
    "    def __init__(self, in_, out, kernel_size, actnorm, padding = 0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_, out, kernel_size, padding = padding, bias = False)\n",
    "        self.actnorm = actnorm\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        out = self.conv(input_)\n",
    "        out, _ = self.actnorm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.initialized = False\n",
    "        self.weights = nn.Parameter(torch.Tensor(c))\n",
    "        self.bias = nn.Parameter(torch.Tensor(c))\n",
    "        \n",
    "    def forward(self, inp, logdet = 0):\n",
    "        if not self.initialized:\n",
    "            c = self.shape[-1]\n",
    "            self.weights.data = torch.log(1/inp.transpose(0, 1).contiguous().view(c, -1).std(1))\n",
    "            self.bias.data = -(inp * self.weights[..., None, None]).transpose(0, 1).contiguous().view(c, -1).mean(1)\n",
    "            self.initialized = True\n",
    "        \n",
    "        return (inp + self.bias[..., None, None]) * torch.exp(self.weights[..., None, None]), logdet + self.shape[0] * self.shape[1] * torch.sum(torch.abs(self.weights))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        return out / torch.exp(self.weights[..., None, None]) - self.bias[..., None, None]\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.shape = (h, w, c)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(np.linalg.qr(np.random.randn(c, c))[0]).float())\n",
    "                                    \n",
    "    def forward(self, inp, logdet):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (inp, self.weight)), logdet + self.shape[0] * self.shape[1] * torch.slogdet(self.weight)[1]\n",
    "\n",
    "    def reverse(self, out):\n",
    "        return torch.einsum(\"abcd,eb->aecd\", (out, torch.inverse(self.weight)))\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, h, w, c):\n",
    "        super().__init__()\n",
    "        self.NN = nn.Sequential(\n",
    "            ZeroBiasInitConv(c//2, 128, 3, ActNorm(h, w, 128), padding=1),\n",
    "            nn.ReLU(),\n",
    "            ZeroBiasInitConv(128, 128, 1, ActNorm(h, w, 128)),\n",
    "            nn.ReLU(),\n",
    "            WeirdConv(128, c)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp, logdet):        \n",
    "        x_a, x_b = torch.chunk(inp, 2, dim=1)\n",
    "        pred = self.NN(x_b)\n",
    "        log_s = pred[:, ::2]\n",
    "        t = pred[:, 1::2]\n",
    "        s = torch.sigmoid(log_s + 2)\n",
    "        y_a = s * (x_a + t)\n",
    "        return torch.cat([y_a, x_b], dim = 1), logdet + torch.sum(torch.log(s))\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        y_a, y_b = torch.chunk(out, 2, dim=1)\n",
    "        pred = self.NN(y_b)\n",
    "        log_s = pred[:, ::2]\n",
    "        t = pred[:, 1::2]\n",
    "        s = torch.sigmoid(log_s + 2)\n",
    "        x_a = y_a / s - t\n",
    "        return torch.cat([x_a, y_b], dim = 1)\n",
    "    \n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, K, h, w, c):\n",
    "        super().__init__()\n",
    "        self.k = K\n",
    "        self.actnorms = nn.ModuleList(ActNorm(h, w, c) for i in range(K))\n",
    "        self.invconvs = nn.ModuleList(InvertibleConv(h, w, c) for i in range(K))\n",
    "        self.couplings = nn.ModuleList(AffineCoupling(h, w, c) for i in range(K))\n",
    "        \n",
    "    def forward(self, inp, logdet, z):        \n",
    "        for i in range(self.k):\n",
    "            inp, logdet = self.actnorms[i](inp, logdet)\n",
    "            inp, logdet = self.invconvs[i](inp, logdet)\n",
    "            inp, logdet = self.couplings[i](inp, logdet)\n",
    "        \n",
    "        return inp, logdet, z\n",
    "    \n",
    "    def reverse(self, inp, z):\n",
    "        for i in range(self.k)[::-1]:\n",
    "            inp = self.couplings[i].reverse(inp)\n",
    "            inp = self.invconvs[i].reverse(inp)\n",
    "            inp = self.actnorms[i].reverse(inp)\n",
    "\n",
    "        return inp, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_unshuffle(input_, block_size=2):\n",
    "    b, c, h, w = input_.shape\n",
    "\n",
    "    assert h % block_size == 0 and w % block_size == 0,\\\n",
    "        f\"Shape must be divisible by block_size, got {input_.shape}\"\n",
    "\n",
    "    oc = c * block_size * block_size;\n",
    "    oh = h // block_size;\n",
    "    ow = w // block_size;\n",
    "\n",
    "    input_reshaped = input_.view(b, c, oh, block_size, ow, block_size)\n",
    "    return input_reshaped.permute(0, 1, 3, 5, 2, 4).reshape(b, oc, oh, ow)\n",
    "    \n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, block_size=2):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size \n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        return pixel_unshuffle(input_, self.block_size), logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return F.pixel_shuffle(input_, self.block_size), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(nn.Module):\n",
    "    def forward(self, input_, logdet, z):\n",
    "        out = input_[:, ::2]\n",
    "        zi = input_[:, 1::2]\n",
    "\n",
    "        return out, logdet, z + [zi]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        out = torch.zeros_like(torch.cat([input_, z[-1]], 1))\n",
    "        out[:, ::2] = input_\n",
    "        out[:, 1::2] = z[-1]\n",
    "        \n",
    "        return out, z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikelihoodOld(nn.Module): \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.norm = torch.distributions.MultivariateNormal(torch.zeros(size).to(device), torch.eye(size).to(device))\n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        batch = input_.shape[0]\n",
    "        comb_z = torch.cat([z_.contiguous().view(batch, -1) for z_ in z + [input_]], 1)\n",
    "\n",
    "        return None, (logdet + torch.sum(self.norm.log_prob(comb_z))) / batch, z + [input_]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return z[-1], z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikelihood(nn.Module): \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, input_, logdet, z):\n",
    "        batch = input_.shape[0]\n",
    "        pdf = (2*np.pi)**(-self.size/2) * torch.exp(-0.5*sum([(z_**2).sum((1, 2, 3)) for z_ in z + [input_]])).sum()\n",
    "\n",
    "        return None, (logdet + pdf) / batch, z + [input_]\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        return z[-1], z[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowModel(nn.Module):\n",
    "    def __init__(self, h, w, c, K, L):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            self.layers.append(Squeeze())\n",
    "            self.layers.append(Flow(K, h//(2**i), w//(2**i), c*2**(i+1)))\n",
    "            self.layers.append(Split())\n",
    "\n",
    "        self.layers.append(Squeeze())\n",
    "        self.layers.append(Flow(K, h//(2**L), w//(2**L), c*2**(L+1)))\n",
    "        self.layers.append(LogLikelihood(h*w*c))\n",
    "        \n",
    "    def forward(self, input_, logdet, z):\n",
    "        for layer in self.layers:\n",
    "            input_, logdet, z = layer(input_, logdet, z)\n",
    "\n",
    "        return input_, logdet, z\n",
    "    \n",
    "    def reverse(self, input_, z):\n",
    "        for layer in self.layers[::-1]:\n",
    "            input_, z = layer.reverse(input_, z)\n",
    "\n",
    "        return input_, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = list(train_loader)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def train_epoch(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.train()\n",
    "    batch_iter = tqdm(train_loader)\n",
    "    for x_batch, _ in batch_iter:\n",
    "        data = x_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, logdet, z = model(data, 0, [])\n",
    "        \n",
    "        loss = -logdet + np.log(n_bins) * np.prod(x_batch.shape[1:])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        batch_iter.set_postfix(loss=loss)\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "def plot_history(train_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    plt.xlabel('train steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def interpolate(ind_1, ind_2, p):\n",
    "    img = train_dataset[ind_1][0]\n",
    "    out, logdet, z = model.forward(img[None, ...].to(device), 0, [])\n",
    "    img2 = train_dataset[ind_2][0]\n",
    "    out2, logdet2, z2 = model.forward(img2[None, ...].to(device), 0, [])\n",
    "    diffs = [z2[i] - z[i] for i in range(len(z))]\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    for i in range(p + 1):\n",
    "        z_i = [z[j] + diffs[j] * i / p for j in range(len(z))]\n",
    "        res = model.reverse(None, z_i)\n",
    "        fig.add_subplot(1, p + 1, i + 1)\n",
    "        plt.imshow(res[0].permute(0, 2, 3, 1).squeeze().detach().cpu().numpy())\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def train(model, opt, n_epochs):\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_epoch(model, opt, batchsize=batch_size)\n",
    "        train_log.extend(train_loss)\n",
    "\n",
    "        clear_output()\n",
    "        plot_history(train_log)\n",
    "        interpolate(np.random.randint(0, len(train_dataset)), np.random.randint(0, len(train_dataset)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlowModel(32, 32, 1, 32, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d75f98652545b5a72faa24874925dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, optim.Adam(model.parameters()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(x):\n",
    "    return np.clip(np.floor((x + 0.5) * n_bins), 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_dataset[41][0]\n",
    "out, logdet, z = model.forward(img[None, ...].to(device), 0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = train_dataset[7643][0]\n",
    "out2, logdet2, z2 = model.forward(img2[None, ...].to(device), 0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = torch.distributions.MultivariateNormal(torch.zeros(1024).to(device), torch.eye(1024).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.log_prob(torch.zeros_like(torch.cat([z_.contiguous().view(1, -1) for z_ in z], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.log_prob(torch.cat([z_.contiguous().view(1, -1) for z_ in z], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat([z_.contiguous().view(1, -1) for z_ in z], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = torch.distributions.MultivariateNormal(torch.zeros(1024).to(device), torch.eye(1024).to(device))\n",
    "b = []\n",
    "for i in range(1000):\n",
    "    a = norm.sample()\n",
    "    b.append(torch.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(1000):\n",
    "    img = train_dataset[i][0]\n",
    "    out, logdet, z = model.forward(img[None, ...].to(device), 0, [])\n",
    "    b.append(torch.norm(torch.cat([z_.contiguous().view(1, -1) for z_ in z], 1)).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(1000):\n",
    "    img = train_dataset[i][0]\n",
    "    img = torch.rand_like(img)\n",
    "    out, logdet, z = model.forward(img[None, ...].to(device), 0, [])\n",
    "    b.append(torch.norm(torch.cat([z_.contiguous().view(1, -1) for z_ in z], 1)).detach().cpu().numpy())\n",
    "plt.hist(b, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = [z2[i] - z[i] for i in range(len(z))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "for i in range(p + 1):\n",
    "    z_i = [z[j] + diffs[j] * i / p for j in range(len(z))]\n",
    "    res = model.reverse(None, z_i)\n",
    "    fig.add_subplot(1, p + 1, i + 1)\n",
    "    plt.imshow(res[0].squeeze().detach().cpu().numpy())\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_z = [z_ + torch.zeros_like(z_).uniform_(0, 1./10) for z_ in z]\n",
    "res = model.reverse(None, new_z)\n",
    "plt.imshow(res[0].squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z[-1][-1, -1, -1, -1] += 0.1\n",
    "res = model.reverse(None, z)\n",
    "plt.imshow(res[0].squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
